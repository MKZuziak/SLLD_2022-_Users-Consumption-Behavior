# [Statistical Learning and Large Data - Web Application Consumption]{.smallcaps}

---
title: "Statistical Learning and Large Data - Web Application Consumption"
auhor: "Maciej Zuziak & Roberto Casaluce"
date: "14/04/2022"
output:
  pdf_document:
    toc: true
  html_document: default
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
library(mvtnorm)
library(factoextra)
library(scales)
library(ellipse)
library(corrplot)
library(ggplot2)
library(dplyr)
library(rrcov)
library(stats)
library(lares)
```

## 1. [Data Exploration & Preparation]{.smallcaps}

```{r}
head(Users.Consumption.Behavior._2019)
```

```{r}
data <- Users.Consumption.Behavior._2019
head(data)
data[,114]
```

```{r}
# Adding a column based on other column:
data <- data%>% mutate(Target = case_when(cluster == "0" ~ "Low",
    cluster == "1" ~ "Medium",
    cluster==  "2" ~ "High"
  ))
```

```{r}
sum(is.na(data))
sum(is.null(data))
```

```{r}
dict <- sapply(data, n_distinct)
print(class(dict))
dict
```

```{r}
# Detaching column containing clusters (column no. 114)
Users_data_overview <- data[,1:113]
data_copy <- Users_data_overview
data_copy
```

Because it was observed, that some categories contain abundance of
values equal 0 (no app consumption) registered for that user, we have
decided to investigate this matter closer.

```{r}
column_names <- c(names(data_copy))
column_names = column_names[2:113]
columns_overview = data.frame(df=column_names)

missing_values = c()
for(i in 2:ncol(Users_data_overview)){
  # Null_values(0)
  sums_zero = sum(data_copy[, i] == 0)
  missing_val = sums_zero
  missing_values = append(missing_values, missing_val)
}

columns_overview$null_values <- c(missing_values)
sorted_overview <-columns_overview[order(columns_overview$null_values, decreasing=FALSE),]
sorted_overview

hist(sorted_overview$null_values,
     main = "Histogram for missing values",
     xlab = "No. of missing values",
     border = "blue",
     col = "green")
```

The histogram presented above clearly indicates that there is a problem
with some categories included in the dataset. Around 30% of the features
does not contain values from around 1200 to 1400 users. Another 30% of
features does not contain values from around 800 1200 users. From that
point onward, there are two possible solutions for that problem:

-   We could remove 60% of the features that are above some
    missing-value threshold.

-   We could perform a PCA on the whole dataset, thus eliminating
    unnecessary features that do not contain much information about
    users behavior.

For now I would prefer the first option - but its open to debate what
behavior we may adapt.

```{r}
uniquelength <- sapply(data,function(x) length(unique(x)))
data <- subset(data, select=uniquelength>2)
data
```

```{r}
uniquelength <- sapply(data,function(x) length(unique(x)))
data <- subset(data, select=uniquelength>2)
data
dict <- sapply(data, n_distinct)
print(class(dict))
dict[0:106]
hist(dict[0:106],
     main = "Histogram for missing values",
     xlab = "No. of missing values",
     border = "blue",
     col = "green")
```

```{r}
colClean <- function(x){ colnames(x) <- gsub("_occupation", "", colnames(x)); x }
data <- colClean(data)
data
```

```{r}
#correlation for just a few features
library(corrplot)
M<-cor(data[,2:5])
head(round(M,2))
#first type of correlation plot
corrplot(M, method="circle")
#second type of correlation plot
corrplot(M, method="number")

###Removing highly correlated variables
###Removing highly correlated variable#### It needs to be tested
cor_matrix <- cor(data[,1:107])  # Correlation matrix
cor_matrix
```

### 1.2. [Summary]{.smallcaps}

In this section, the following was performed:

1.  General EDA to get acquainted with the data set basic features and
    characteristic.

2.  Data sanitation procedure, during which:

    -   We have performed search for missing values (no abnormalities
        detected).

    -   Inspected the amount of information contained by some features -
        thus removing features that did not contain any entries or where
        the number of entries was smaller then 2.

    -   Check the the correlation of features and as a consequence,
        removed features that were highly correlated.

    -   Added an additional target categorical feature to better explain
        the data set. Entries from relevant clusters were marked as
        follow:

        -   Cluster 0 - Low

        -   Cluster 1 - Medium

        -   Cluster 2 - High

3.  As a consequence of our actions, the data set was transformed from
    114 dimensions to 107 dimensions.

## 2. [Principal Component Analysis]{.smallcaps}

### 2.1. [PCA Introduction]{.smallcaps}

#### 2.1.1. [Min-Max Normalization]{.smallcaps}

$$
x' = \frac{x - min(x)}{max(x) - min(x)}
$$

```{r}
exemplary_entry = data[, c('HTTP_time', 'Amazon_time')]

norm_minmax <- function(x){
  (x - min(x)) / (max(x) - min(x))
}

exemplary_entry <- as.data.frame(lapply(exemplary_entry, norm_minmax))
head(exemplary_entry)
ggplot()+
  geom_point(exemplary_entry, mapping = aes(x=HTTP_time, y=Amazon_time, color='blue'))
```

#### 2.1.2. [Introduction to dimensional reduction using the sample's mean]

```{r}
# Using the sample mean as a 0-dimensional reduction of data on the example of 'HTTP_time' and 'Amazon_time'
med <- colMeans(exemplary_entry) #PC 0

#plotting the mean
plot(exemplary_entry, asp=1)
points(med[1], med[2], col='red', pch=16)

plot(exemplary_entry, asp=1)
points(med[1], med[2], col='red', pch=16)
for(i in 1:200)
  lines(rbind(exemplary_entry[i,], med), col='red')
```

```{r}
# For the horizontal axis y = med[2]
plot(exemplary_entry, asp=1)
points(med[1], med[2], col='red', pch=16)
abline(h=med[2], lty=2) # h - the y-values(s) for horizontal line(s)
points(exemplary_entry[,1], rep(med[2], length(exemplary_entry[,1])), col='red')
var(exemplary_entry[,1])

# For the vertical-axis, x = med[1]
abline(v=med[1], lty=2) # v - the x value(s) for horizontal line(s)
points(rep(med[1], length(exemplary_entry[,1])), exemplary_entry[,2], col='blue')
var(exemplary_entry[,2])
```

### 2.2. [PCA on the full data set]{.smallcaps}

In this section, we try to run PCA on the data set to obtain some
further information about further dimensionality reduction that may be
possible.

```{r}
data_pca <- data[,2:106]
res <- prcomp(data_pca, scale = TRUE)
get_eig(res)
fviz_eig(res)
fviz_eig(res, ncp = 25)
```

```{r}
plot(get_eig(res)$cumulative.variance.percent, 
     type='b', axes=F, xlab='Dimensions', ylab='cumulative PVE', ylim=c(0,100))
abline(h=100, col=alpha('blue',0.5))
abline(h=80, lty=2, col='red', lwd=2) # thresholding
box()
axis(2, at=0:100,labels=0:100)
axis(1,at=1:ncol(data_pca),labels=1:ncol(data_pca))
grid()
```
The following three plots are giving us basic information on the
variance preserved by the principal components. While the elbow on the
scree plot is visible around n(dimensions) equal to 5 or 6, we must also
inspect the graph of cumulative preserved variance explained. Ideally,
we should aim for around 70 to 80% of PVE. This would require preserving
around 47 dimensions of the data set.
```{r}
varpca <- get_pca_var(res)
varpca <- varpca$cos2
varpca <- as.data.frame(varpca)
varpca <- t(varpca)
varpca <- as.data.frame(varpca)
corr_cross(varpca, max_pvalue = 0.05, top = 20) #Looked at this, maybe we should drop data and leave only time?
```
```{r}
fviz_pca_var(res,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), geom = c('point'),
             repel = TRUE, title="PCA:2D - Contribution plot of the all variables")    # Avoid text overlapping

fviz_pca_var(res,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, select.var = list(contrib = 10), title="PCA:2D - Contribution plot of the top 10 contributing active variables [arrow + text]")

fviz_pca_var(res,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, geom = c('point', 'text'), select.var = list(contrib = 10), title="PCA:2D - Contribution plot of the top 10 contributing active variables [point + text]")

fviz_pca_biplot(res,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, select.var = list(contrib = 10), select.ind = list(contrib = 30), title="PCA:2D - Contribution plot of the top 10 contributing active variables and 30 individuals")

fviz_pca_biplot(res,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = FALSE, title="PCA:2D - Complete Biplot")
```

One last experiment provides for using Robust PCA as an alternative to
"standard" PCA and comparing those two methods.

```{r}
pcarob <- PcaHubert(data_pca, k=4, alpha=0.75, scale = TRUE)
par(mfrow=c(1,2))
plot(pcarob)

pcarob2 <- PcaHubert(data_pca, k=2)
plot(pcarob2)

```

```{r}
screeplot(pcarob, main='Robust PCA')
screeplot(res, main='Standard PCA') #Res is a standard PCA performed by prcomp(data_pca, scale = TRUE)
```

### 2.3. [Multidimensional Scaling]{.smallcaps}

```{r}
#data_dist <- dist(data_pca)
#data_dist
```

